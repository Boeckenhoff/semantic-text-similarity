{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sts-bockenhoff-urcelay_final_version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IHLT. Semantic Textual Similarity Project"
      ],
      "metadata": {
        "id": "SQPir9qxpF_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Authors**: Bernhard Bockenhoff and Luc√≠a Urcelay\n",
        "\n",
        "**Abstract**: According to the task description paper, \"Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts\". This project aims to compute the semantic textual similarity between pairs of sentences from the SMT dataset so that it best resembles the gold standard score provided by linguistic experts.\n",
        "\n",
        "To do this, firstly, several submissions from the SemEval 2012 workshops have been analysed in order to get a general idea of different possible approaches. Secondly, \"UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures\" submission from authors Daniel Bar, Chris Biemann, Iryna Gurevych and Torsten Zesch has been chosen as reference.\n",
        "\n",
        "As for the pipeline of our approach, this can be summed up in the following steps:\n",
        "\n",
        "\n",
        "1.   *Resource importation*: nltk, pandas, numpy, sklearn...\n",
        "2.   *Train and test data loading*: six datasets, three for training and three for testing, have been loaded and concatenated with each other (SMTeuropearl, MSRvid, MSRpar)\n",
        "3.   *Preprocessing*: convert everything to lowercase, filter non alphanumeric characters, tokenize and lemmatize\n",
        "4.   *Feature engineering*: definition of different functions (similarity measures, sentence processing and manipulation, string similarities, semantic similarities...)\n",
        "5.   *Feature extraction*: computation of similarity of processed sentences by the functions defined in the previous step\n",
        "6.   *Feature combination*: in this step we combine all the similarity scores that have been obtained using a Support Vector Machine classifier and calculate Pearson Correlation for the testing dataset\n",
        "\n",
        "**Outcome**: The result is in the form of a similarity given by the **Pearson Correlation**: being the best value obtained of **0.7773**"
      ],
      "metadata": {
        "id": "D3plNxPgnblP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaZMnzyldDC7"
      },
      "source": [
        "# Import resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoZvN3PDc0gJ",
        "outputId": "8ca085ba-f205-43dc-c18c-d7d7a4c5467a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wdoFH12dHCk",
        "outputId": "f80f9f7e-5114-4946-f497-072baccbfd1e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet_ic')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import wordnet_ic\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "import spacy\n",
        "en = spacy.load('en_core_web_sm')\n",
        "sw_spacy = en.Defaults.stop_words\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import jaccard_score\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "\n",
        "import re\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVgi-ANtdXYW"
      },
      "source": [
        "# Load training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVaCHzBddaC7"
      },
      "source": [
        "# Function for reading MSRpar dataset, due to error in parsing with pd.read_csv\n",
        "\n",
        "def load_dataframe(input_filepath):\n",
        "  current_file_path = input_filepath\n",
        "  try:\n",
        "    data = []\n",
        "    with open(input_filepath, 'r') as f:\n",
        "      lines = f.read().splitlines()\n",
        "      for line in lines:\n",
        "        data.append(line.split(\"\\t\"))\n",
        "    df = pd.DataFrame(data, columns = [0, 1])\n",
        "    \n",
        "  except Exception as e:\n",
        "    raise Exception(f\"ERROR while reading {current_file_path}:\\n\\t{e}\")\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqevYFw1dlpz"
      },
      "source": [
        "##########################################################################################################\n",
        "                                  # Load training data\n",
        "##########################################################################################################\n",
        "\n",
        "dfSMT = pd.read_csv('/content/drive/My Drive/IHLT/finalproject/Data/Train/SMTeuroparl/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "dfSMT.columns = ['Sen1', 'Sen2']\n",
        "dfSMT['gs'] = pd.read_csv('/content/drive/My Drive/IHLT/finalproject/Data/Train/SMTeuroparl/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "\n",
        "dfMSRv = pd.read_csv('/content/drive/My Drive/IHLT/finalproject/Data/Train/MSRvid/STS.input.MSRvid.txt',sep='\\t',header=None)\n",
        "dfMSRv.columns = ['Sen1', 'Sen2']\n",
        "dfMSRv['gs'] = pd.read_csv('/content/drive/My Drive/IHLT/finalproject/Data/Train/MSRvid/STS.gs.MSRvid.txt',sep='\\t',header=None)\n",
        "\n",
        "dfMSRp = load_dataframe('/content/drive/My Drive/IHLT/finalproject/Data/Test/MSRpar/STS.input.MSRpar.txt')\n",
        "dfMSRp.columns = ['Sen1', 'Sen2']\n",
        "dfMSRp['gs'] = pd.read_csv('/content/drive/My Drive/IHLT/finalproject/Data/Test/MSRpar/STS.gs.MSRpar.txt',sep='\\t',header=None)\n",
        "\n",
        "#Here we can concetenate the datasets\n",
        "data_train = pd.concat([dfSMT, dfMSRv, dfMSRp])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ULeGQ6JheHSI",
        "outputId": "37441dbc-507d-48ec-a8dd-b2071f351e64"
      },
      "source": [
        "# Training data\n",
        "\n",
        "data_train.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sen1</th>\n",
              "      <th>Sen2</th>\n",
              "      <th>gs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In Nigeria, Chevron has been accused by the Al...</td>\n",
              "      <td>In Nigeria, the whole ijaw indigenous showed C...</td>\n",
              "      <td>4.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I know that in France they have had whole herd...</td>\n",
              "      <td>I know that in France, the principle of slaugh...</td>\n",
              "      <td>4.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Unfortunately, the ultimate objective of a Eur...</td>\n",
              "      <td>Unfortunately the final objective of a Europea...</td>\n",
              "      <td>4.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The right of a government arbitrarily to set a...</td>\n",
              "      <td>The right for a government to draw aside its c...</td>\n",
              "      <td>4.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The House had also fought, however, for the re...</td>\n",
              "      <td>This Parliament has also fought for this reduc...</td>\n",
              "      <td>4.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Sen1  ...    gs\n",
              "0  In Nigeria, Chevron has been accused by the Al...  ...  4.20\n",
              "1  I know that in France they have had whole herd...  ...  4.25\n",
              "2  Unfortunately, the ultimate objective of a Eur...  ...  4.80\n",
              "3  The right of a government arbitrarily to set a...  ...  4.80\n",
              "4  The House had also fought, however, for the re...  ...  4.00\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 460
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mnRMyDMdsP8"
      },
      "source": [
        "##########################################################################################################\n",
        "                                  # Load testing data\n",
        "##########################################################################################################\n",
        "\n",
        "dfSMT_test = pd.read_csv('/content/drive/My Drive/IHLT/finalproject/Data/Test/SMTeuroparl/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "dfSMT_test.columns = ['Sen1', 'Sen2']\n",
        "dfSMT_test['gs'] = pd.read_csv('/content/drive/My Drive/IHLT/finalproject/Data/Test/SMTeuroparl/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "\n",
        "dfMSRv_test = pd.read_csv('/content/drive/My Drive/IHLT/finalproject/Data/Test/MSRvid/STS.input.MSRvid.txt',sep='\\t',header=None)\n",
        "dfMSRv_test.columns = ['Sen1', 'Sen2']\n",
        "dfMSRv_test['gs'] = pd.read_csv('/content/drive/My Drive/IHLT/finalproject/Data/Test/MSRvid/STS.gs.MSRvid.txt',sep='\\t',header=None)\n",
        "\n",
        "dfMSRp_test = load_dataframe('/content/drive/My Drive/IHLT/finalproject/Data/Test/MSRpar/STS.input.MSRpar.txt')\n",
        "dfMSRp_test.columns = ['Sen1', 'Sen2']\n",
        "dfMSRp_test['gs'] = pd.read_csv('/content/drive/My Drive/IHLT/finalproject/Data/Test/MSRpar/STS.gs.MSRpar.txt',sep='\\t',header=None)\n",
        "\n",
        "data_test = pd.concat([dfSMT_test, dfMSRv_test, dfMSRp_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "brAw1V4XeJYt",
        "outputId": "2a0b7c80-43c9-40bd-bf7c-2c56bb6a09da"
      },
      "source": [
        "# Testing data\n",
        "\n",
        "data_test.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sen1</th>\n",
              "      <th>Sen2</th>\n",
              "      <th>gs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The leaders have now been given a new chance a...</td>\n",
              "      <td>The leaders benefit aujourd' hui of a new luck...</td>\n",
              "      <td>4.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Amendment No 7 proposes certain changes in the...</td>\n",
              "      <td>Amendment No 7 is proposing certain changes in...</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Let me remind you that our allies include ferv...</td>\n",
              "      <td>I would like to remind you that among our alli...</td>\n",
              "      <td>4.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The vote will take place today at 5.30 p.m.</td>\n",
              "      <td>The vote will take place at 5.30pm</td>\n",
              "      <td>4.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
              "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Sen1  ...    gs\n",
              "0  The leaders have now been given a new chance a...  ...  4.50\n",
              "1  Amendment No 7 proposes certain changes in the...  ...  5.00\n",
              "2  Let me remind you that our allies include ferv...  ...  4.25\n",
              "3        The vote will take place today at 5.30 p.m.  ...  4.50\n",
              "4  The fishermen are inactive, tired and disappoi...  ...  5.00\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 462
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "wusJuNtWa2me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmas(sen):\n",
        "  t_list = nltk.word_tokenize(sen) \n",
        "  t_POS_list = nltk.pos_tag(t_list)\n",
        "  li = [lemmatize(pair) for pair in t_POS_list]\n",
        "  st = ' '.join([str(item) for item in li])\n",
        "  return (st)\n",
        "\n",
        "def lemmatize(p):\n",
        "  translate = {'N': 'n', 'V': 'v','J': 'a','R': 'r'}\n",
        "  if p[1][0] in {'N','V','J','R'}:\n",
        "      return wnl.lemmatize(p[0].lower(), pos= translate[p[1][0]])  \n",
        "  return p[0]"
      ],
      "metadata": {
        "id": "4SBA0nMhR8Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(sen):\n",
        "  sen = sen.lower()\n",
        "  sen = re.sub('[^0-9a-zA-Z ]+', '', sen)\n",
        "  sen = lemmas(sen)\n",
        "  return(sen)\n",
        "\n",
        "data_train['Sen1'] = data_train.apply(lambda row: (preprocessing(row['Sen1'])), axis=1)\n",
        "data_train['Sen2'] = data_train.apply(lambda row: (preprocessing(row['Sen2'])), axis=1)\n",
        "\n",
        "data_test['Sen1'] = data_test.apply(lambda row: (preprocessing(row['Sen1'])), axis=1)\n",
        "data_test['Sen2'] = data_test.apply(lambda row: (preprocessing(row['Sen2'])), axis=1)\n"
      ],
      "metadata": {
        "id": "6sijfUUYJoL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "D-hR01nzPXz7",
        "outputId": "61d79e2d-e962-436e-cfd8-1b023d96e9ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sen1</th>\n",
              "      <th>Sen2</th>\n",
              "      <th>gs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>in nigeria chevron have be accuse by the allij...</td>\n",
              "      <td>in nigeria the whole ijaw indigenous show chev...</td>\n",
              "      <td>4.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i know that in france they have have whole her...</td>\n",
              "      <td>i know that in france the principle of slaught...</td>\n",
              "      <td>4.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>unfortunately the ultimate objective of a euro...</td>\n",
              "      <td>unfortunately the final objective of a europea...</td>\n",
              "      <td>4.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the right of a government arbitrarily to set a...</td>\n",
              "      <td>the right for a government to draw aside its c...</td>\n",
              "      <td>4.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the house have also fight however for the redu...</td>\n",
              "      <td>this parliament have also fight for this reduc...</td>\n",
              "      <td>4.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Sen1  ...    gs\n",
              "0  in nigeria chevron have be accuse by the allij...  ...  4.20\n",
              "1  i know that in france they have have whole her...  ...  4.25\n",
              "2  unfortunately the ultimate objective of a euro...  ...  4.80\n",
              "3  the right of a government arbitrarily to set a...  ...  4.80\n",
              "4  the house have also fight however for the redu...  ...  4.00\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 465
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ6lC8KYeNNq"
      },
      "source": [
        "# Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcsqk4K8eMqd"
      },
      "source": [
        "################################  Similarity Measures        ################################\n",
        "\n",
        "def soft_sim(x,y, alpha, bias):\n",
        "  setx = set(x)\n",
        "  sety = set(y)\n",
        "  return (len( setx.intersection(sety))+ bias)/(alpha* max(len(setx),len(sety))+(1-alpha)*min(len(setx),len(sety)))\n",
        "\n",
        "def jac_sim (x, y ):\n",
        "  setx = set(x)\n",
        "  sety = set(y)\n",
        "  try:\n",
        "    jac_sim = len( setx.intersection(sety)) / len(setx.union(sety)) \n",
        "  except:\n",
        "    jac_sim = 0\n",
        "  return jac_sim\n",
        "\n",
        "def containment(x,y):\n",
        "  setx = set(x)\n",
        "  sety = set(y)\n",
        "  try:\n",
        "    containment = len(setx.intersection(sety)) / len(setx) \n",
        "  except:\n",
        "    containment = 0\n",
        "  return containment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ16k1cbhPA9"
      },
      "source": [
        "################################    Sentence Processing    ################################\n",
        "def stopword_removal(sen):\n",
        "  t_list = nltk.word_tokenize(sen) \n",
        "  return [word for word in t_list if word.lower() not in sw_spacy]\n",
        "\n",
        "\n",
        "def ml_syns(sen):\n",
        "  t_list = nltk.word_tokenize(sen) \n",
        "  t_POS_list = nltk.pos_tag(t_list)\n",
        "  result= [ml_syn(pair, t_list) for pair in t_POS_list]\n",
        "  return result\n",
        "\n",
        "def ml_syn(p, context):\n",
        "  translate = {'N': 'n', 'V': 'v','J': 'a','R': 'r'}\n",
        "  if p[1][0] in {'N','V','J','R'}:\n",
        "    if nltk.wsd.lesk(context, p[0].lower(), translate[p[1][0]]) is None:\n",
        "      return lemmatize(p)\n",
        "    else:\n",
        "      return nltk.wsd.lesk(context, p[0].lower(), translate[p[1][0]]).name()\n",
        "  return p[0]\n",
        "\n",
        "def numberOfSynset(word):\n",
        "  if not word[1] in [\"DT\",\"PR\",\"CC\"]:\n",
        "    for synset in wn.synsets( word[0], translation[word[1]]):\n",
        "      count = sum([l.count() for l in synset.lemmas()])\n",
        "      if maximum < count:\n",
        "        maximum = count\n",
        "        name = synset\n",
        "  return name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n149hB_ohV19"
      },
      "source": [
        "################################    Word similairty    ################################\n",
        "def pairwise_word_similarity(sen1, sen2, type='res', onlydifference=False):\n",
        "  syns1 = ml_syns1(sen1)\n",
        "  syns2 = ml_syns1(sen2)\n",
        "\n",
        "  if onlydifference:\n",
        "    syns1 = list(set(syns1) - set(syns2))\n",
        "    syns2 = list(set(syns2) - set(syns1))\n",
        "\n",
        "  if type == 'res' or type == 'lin':\n",
        "    syns1 = [x for x in syns1 if x.pos() != 'a' and x.pos() != 'r']\n",
        "    syns2 = [x for x in syns2 if x.pos() != 'a' and x.pos() != 'r']\n",
        "    if (len(syns1) == 0)or (len(syns2) == 0):\n",
        "      return 10\n",
        "\n",
        "  res12 = np.zeros([len(syns1),len(syns2)], dtype=np.float64)\n",
        "  res21 = np.zeros([len(syns2),len(syns1)], dtype=np.float64)\n",
        "\n",
        "  for x, synset1 in enumerate(syns1):   \n",
        "    for y, synset2 in enumerate(syns2):\n",
        "      if type == 'path':\n",
        "        res12[x][y] = synset1.path_similarity(synset2)\n",
        "        res21[y][x] = synset2.path_similarity(synset1)\n",
        "\n",
        "      if type == 'wup':\n",
        "        res12[x][y] = synset1.wup_similarity(synset2)\n",
        "        res21[y][x] = synset2.wup_similarity(synset1)\n",
        "\n",
        "      if type == 'lch':\n",
        "        res[x][y] = synset1.lch_similarity(synset2)\n",
        "        res[y][x] = synset2.lch_similarity(synset1)\n",
        "\n",
        "      if synset1.pos() == synset2.pos():  \n",
        "        try:  \n",
        "          if type == 'lin':\n",
        "            res12[x][y] = synset1.lin_similarity(synset2, brown_ic)\n",
        "            res21[y][x] = synset2.lin_similarity(synset1, brown_ic)\n",
        "\n",
        "          if type == 'res':\n",
        "            res12[x][y] =synset1.res_similarity(synset2, brown_ic)\n",
        "            res21[y][x] =synset2.res_similarity(synset1, brown_ic)\n",
        "\n",
        "        except:\n",
        "          print(synset1.pos())\n",
        "          res12[x][y] = 0\n",
        "          res21[y][x] = 0\n",
        "\n",
        "  non12 = np.nan_to_num(res12)\n",
        "  non21 =np.nan_to_num(res21)\n",
        "  try:\n",
        "    return (np.mean(np.amax(non12, axis=1)) + np.mean(np.amax(non21, axis=1)))/2\n",
        "  except:\n",
        "    return 1\n",
        "\n",
        "\n",
        "def ml_syns1(sen):\n",
        "  t_list = nltk.word_tokenize(sen) \n",
        "  t_POS_list = nltk.pos_tag(t_list)\n",
        "  result= [ml_syn1(pair, t_list) for pair in t_POS_list]\n",
        "  result = [x for x in result if x is not None]\n",
        "  return result\n",
        "\n",
        "def ml_syn1(p, context):\n",
        "  translate = {'N': 'n', 'V': 'v','J': 'a','R': 'r'} #look into translations\n",
        "  if p[1][0] in {'N','V','J','R'}:\n",
        "    if nltk.wsd.lesk(context, p[0].lower(), translate[p[1][0]]) is None:\n",
        "      return \n",
        "    else:\n",
        "      return nltk.wsd.lesk(context, p[0].lower(), translate[p[1][0]])\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MonmyOfGhdSB"
      },
      "source": [
        "def longestCommonSubstringMeasure(string1, string2):\n",
        "  a_offset, b_offset, size = SequenceMatcher(None, string1, string2).find_longest_match(0, len(string1), 0, len(string2))\n",
        "  return (size / len(string2) + size / len(string2))/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubvnRgRIhmZ1"
      },
      "source": [
        "def generate_N_grams(text, ngram=1, stw=False):\n",
        "  if stw:\n",
        "    words=[word for word in text.split(' ') if word not in set(stopwords.words('english'))]\n",
        "  else:\n",
        "    words=[word for word in text.split(' ')]\n",
        "  temp=zip(*[words[i:] for i in range(0,ngram)])\n",
        "  ans=[' '.join(ngram) for ngram in temp]\n",
        "  return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKqc0y37hm7J"
      },
      "source": [
        "def char_N_grams(text, ngram):\n",
        "  letters = [letter for letter in list(text) if letter not in set(' ')]\n",
        "  temp=zip(*[letters[i:] for i in range(0,ngram)])\n",
        "  ans=[' '.join(ngram) for ngram in temp]\n",
        "  return ans\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ryeCAvAndnX"
      },
      "source": [
        "def pos_tag_ngram(sen, ngram):\n",
        "  t_list = nltk.word_tokenize(sen) \n",
        "  t_POS_list = nltk.pos_tag(t_list)\n",
        "  ans = [x[1] for x in t_POS_list]\n",
        "  temp=zip(*[ans[i:] for i in range(0,ngram)])\n",
        "  ans=[' '.join(ngram) for ngram in temp]\n",
        "  return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFTVoCTJeQRn"
      },
      "source": [
        "# Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ECi1EuteSlV"
      },
      "source": [
        "##########################################################################################################\n",
        "                                  # Function for extracting features\n",
        "##########################################################################################################\n",
        "\n",
        "def extract_features(data):\n",
        "  \n",
        "  # Longest Common Substring\n",
        "  data['LCStr'] =  data.apply(lambda row: longestCommonSubstringMeasure(row['Sen1'], row['Sen2']), axis=1)\n",
        "\n",
        "  # Raw Preprocessed\n",
        "  data['soft preprocessed'] =  data.apply(lambda row: soft_sim(row['Sen1'], row['Sen2'], 0.74, -0.06), axis=1)\n",
        "  data['JS preprocessed'] =  data.apply(lambda row: jac_sim(row['Sen1'], row['Sen2']), axis=1)\n",
        "\n",
        "  # Synsets\n",
        "  data['soft synsets'] =  data.apply(lambda row: soft_sim(ml_syns(row['Sen1']),ml_syns(row['Sen2']), 0.74, -0.06), axis=1)\n",
        "  data['JS Synsets'] =  data.apply(lambda row: jac_sim(ml_syns(row['Sen1']), ml_syns(row['Sen2'])), axis=1)\n",
        "\n",
        "  # Character 2-,3-, and 4-grams \n",
        "  data['char 2 gram jac'] =  data.apply(lambda row: jac_sim(char_N_grams(row['Sen1'],2), char_N_grams(row['Sen2'],2)), axis=1)\n",
        "  data['char 3 gram jac'] =  data.apply(lambda row: jac_sim(char_N_grams(row['Sen1'],3), char_N_grams(row['Sen2'],3)), axis=1)\n",
        "  data['char 4 gram jac'] =  data.apply(lambda row: jac_sim(char_N_grams(row['Sen1'],4), char_N_grams(row['Sen2'],4)), axis=1)\n",
        "  data['char 2 gram cont'] =  data.apply(lambda row: containment(char_N_grams(row['Sen1'],2), char_N_grams(row['Sen2'],2)), axis=1)\n",
        "  data['char 3 gram cont'] =  data.apply(lambda row: containment(char_N_grams(row['Sen1'],3), char_N_grams(row['Sen2'],3)), axis=1)\n",
        "  data['char 4 gram cont'] =  data.apply(lambda row: containment(char_N_grams(row['Sen1'],4), char_N_grams(row['Sen2'],4)), axis=1)\n",
        "  data['char 5 gram jac'] =  data.apply(lambda row: jac_sim(char_N_grams(row['Sen1'],5), char_N_grams(row['Sen2'],5)), axis=1)\n",
        "  data['char 6 gram jac'] =  data.apply(lambda row: jac_sim(char_N_grams(row['Sen1'],6), char_N_grams(row['Sen2'],6)), axis=1)\n",
        "  data['char 7 gram jac'] =  data.apply(lambda row: jac_sim(char_N_grams(row['Sen1'],7), char_N_grams(row['Sen2'],7)), axis=1)\n",
        "  data['char 5 gram cont'] =  data.apply(lambda row: containment(char_N_grams(row['Sen1'],5), char_N_grams(row['Sen2'],5)), axis=1)\n",
        "  data['char 6 gram cont'] =  data.apply(lambda row: containment(char_N_grams(row['Sen1'],6), char_N_grams(row['Sen2'],6)), axis=1)\n",
        "  data['char 7 gram cont'] =  data.apply(lambda row: containment(char_N_grams(row['Sen1'],7), char_N_grams(row['Sen2'],7)), axis=1)\n",
        "\n",
        "  # Word 1- and 2-grams (Containment,w/o stopwords)\n",
        "  data['Con 1gram w/o stw'] =  data.apply(lambda row: containment(generate_N_grams(row['Sen1'],1,True), generate_N_grams(row['Sen2'],1,True)), axis=1)\n",
        "  data['Con 2gram w/o stw'] =  data.apply(lambda row: containment(generate_N_grams(row['Sen1'],2,True), generate_N_grams(row['Sen2'],2,True)), axis=1)\n",
        "  data['Con 3gram w/o stw'] =  data.apply(lambda row: containment(generate_N_grams(row['Sen1'],3,True), generate_N_grams(row['Sen2'],3,True)), axis=1)\n",
        "  data['Con 4gram w/o stw'] =  data.apply(lambda row: containment(generate_N_grams(row['Sen1'],4,True), generate_N_grams(row['Sen2'],4,True)), axis=1)\n",
        "\n",
        "  # Word 1-,3-, and 4-grams (Jaccard)\n",
        "  data['JS 1gram'] =  data.apply(lambda row: jac_sim(generate_N_grams(row['Sen1'],1), generate_N_grams(row['Sen2'],1)), axis=1)\n",
        "  data['JS 3gram'] =  data.apply(lambda row: jac_sim(generate_N_grams(row['Sen1'],3), generate_N_grams(row['Sen2'],3)), axis=1)\n",
        "  data['JS 4gram'] =  data.apply(lambda row: jac_sim(generate_N_grams(row['Sen1'],4), generate_N_grams(row['Sen2'],4)), axis=1)\n",
        "  \n",
        "  # Word 1-,3-, and 4-grams (Jaccard, w/o stopwords)\n",
        "  data['JS 1gram w/o stw'] =  data.apply(lambda row: jac_sim(generate_N_grams(row['Sen1'],1,True), generate_N_grams(row['Sen2'],1,True)), axis=1)\n",
        "  data['JS 3gram w/o stw'] =  data.apply(lambda row: jac_sim(generate_N_grams(row['Sen1'],3,True), generate_N_grams(row['Sen2'],3,True)), axis=1)\n",
        "  data['JS 4gram w/o stw'] =  data.apply(lambda row: jac_sim(generate_N_grams(row['Sen1'],4,True), generate_N_grams(row['Sen2'],4,True)), axis=1)\n",
        "\n",
        "  # Pos Tag Ngram\n",
        "  data['POS 1gram jac'] =  data.apply(lambda row: jac_sim(pos_tag_ngram(row['Sen1'],1), pos_tag_ngram(row['Sen2'],1)), axis=1)\n",
        "  data['POS 2gram jac'] =  data.apply(lambda row: jac_sim(pos_tag_ngram(row['Sen1'],2), pos_tag_ngram(row['Sen2'],2)), axis=1)\n",
        "  data['POS 3gram jac'] =  data.apply(lambda row: jac_sim(pos_tag_ngram(row['Sen1'],3), pos_tag_ngram(row['Sen2'],3)), axis=1)\n",
        "  \n",
        "  data['POS 1gram cont'] =  data.apply(lambda row: containment(pos_tag_ngram(row['Sen1'],1), pos_tag_ngram(row['Sen2'],1)), axis=1)\n",
        "  data['POS 2gram cont'] =  data.apply(lambda row: containment(pos_tag_ngram(row['Sen1'],2), pos_tag_ngram(row['Sen2'],2)), axis=1)\n",
        "  data['POS 3gram cont'] =  data.apply(lambda row: containment(pos_tag_ngram(row['Sen1'],3), pos_tag_ngram(row['Sen2'],3)), axis=1)\n",
        "\n",
        "  # Pairwise Word Simmilarity\n",
        "  data['res dif'] =  data.apply(lambda row: pairwise_word_similarity(row['Sen1'],row['Sen2'],'res', True), axis=1)\n",
        "  data['lin'] =  data.apply(lambda row: pairwise_word_similarity(row['Sen1'],row['Sen2'],'lin'), axis=1)\n",
        "  data['wup'] =  data.apply(lambda row: pairwise_word_similarity(row['Sen1'],row['Sen2'],'wup'), axis=1)\n",
        "\n",
        "  # Stopword Soft Similarity \n",
        "  data['soft stopword'] =  data.apply(lambda row: soft_sim(stopword_removal(row['Sen1']),stopword_removal(row['Sen2']), 0.74, -0.06), axis=1)\n",
        "\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AchDyj5Rfl5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26acf11f-9344-4835-c098-fc288b030a78"
      },
      "source": [
        "##########################################################################################################\n",
        "                                  # Extract features from data\n",
        "##########################################################################################################\n",
        "\n",
        "data_train = extract_features(data_train)\n",
        "data_test = extract_features(data_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "4kRvbe6kjQvW",
        "outputId": "99299fea-4c86-4def-c907-0f7d6dab95bd"
      },
      "source": [
        "data_train.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sen1</th>\n",
              "      <th>Sen2</th>\n",
              "      <th>gs</th>\n",
              "      <th>LCStr</th>\n",
              "      <th>soft preprocessed</th>\n",
              "      <th>JS preprocessed</th>\n",
              "      <th>soft synsets</th>\n",
              "      <th>JS Synsets</th>\n",
              "      <th>char 2 gram jac</th>\n",
              "      <th>char 3 gram jac</th>\n",
              "      <th>char 4 gram jac</th>\n",
              "      <th>char 2 gram cont</th>\n",
              "      <th>char 3 gram cont</th>\n",
              "      <th>char 4 gram cont</th>\n",
              "      <th>char 5 gram jac</th>\n",
              "      <th>char 6 gram jac</th>\n",
              "      <th>char 7 gram jac</th>\n",
              "      <th>char 5 gram cont</th>\n",
              "      <th>char 6 gram cont</th>\n",
              "      <th>char 7 gram cont</th>\n",
              "      <th>Con 1gram w/o stw</th>\n",
              "      <th>Con 2gram w/o stw</th>\n",
              "      <th>Con 3gram w/o stw</th>\n",
              "      <th>Con 4gram w/o stw</th>\n",
              "      <th>JS 1gram</th>\n",
              "      <th>JS 3gram</th>\n",
              "      <th>JS 4gram</th>\n",
              "      <th>JS 1gram w/o stw</th>\n",
              "      <th>JS 3gram w/o stw</th>\n",
              "      <th>JS 4gram w/o stw</th>\n",
              "      <th>POS 1gram jac</th>\n",
              "      <th>POS 2gram jac</th>\n",
              "      <th>POS 3gram jac</th>\n",
              "      <th>POS 1gram cont</th>\n",
              "      <th>POS 2gram cont</th>\n",
              "      <th>POS 3gram cont</th>\n",
              "      <th>res dif</th>\n",
              "      <th>lin</th>\n",
              "      <th>wup</th>\n",
              "      <th>soft stopword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>in nigeria chevron have be accuse by the allij...</td>\n",
              "      <td>in nigeria the whole ijaw indigenous show chev...</td>\n",
              "      <td>4.20</td>\n",
              "      <td>0.175141</td>\n",
              "      <td>0.997391</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.725619</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.578512</td>\n",
              "      <td>0.398936</td>\n",
              "      <td>0.334928</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.551471</td>\n",
              "      <td>0.496454</td>\n",
              "      <td>0.286364</td>\n",
              "      <td>0.246696</td>\n",
              "      <td>0.214592</td>\n",
              "      <td>0.440559</td>\n",
              "      <td>0.391608</td>\n",
              "      <td>0.349650</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.542857</td>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.074074</td>\n",
              "      <td>0.038462</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.521739</td>\n",
              "      <td>0.346154</td>\n",
              "      <td>0.647161</td>\n",
              "      <td>0.701390</td>\n",
              "      <td>0.772420</td>\n",
              "      <td>0.674476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i know that in france they have have whole her...</td>\n",
              "      <td>i know that in france the principle of slaught...</td>\n",
              "      <td>4.25</td>\n",
              "      <td>0.178571</td>\n",
              "      <td>0.964820</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.687213</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.521277</td>\n",
              "      <td>0.364341</td>\n",
              "      <td>0.273973</td>\n",
              "      <td>0.753846</td>\n",
              "      <td>0.610390</td>\n",
              "      <td>0.506329</td>\n",
              "      <td>0.212903</td>\n",
              "      <td>0.176101</td>\n",
              "      <td>0.148148</td>\n",
              "      <td>0.412500</td>\n",
              "      <td>0.354430</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.653846</td>\n",
              "      <td>0.097561</td>\n",
              "      <td>0.048780</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.631904</td>\n",
              "      <td>0.708967</td>\n",
              "      <td>0.805906</td>\n",
              "      <td>0.644252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>unfortunately the ultimate objective of a euro...</td>\n",
              "      <td>unfortunately the final objective of a europea...</td>\n",
              "      <td>4.80</td>\n",
              "      <td>0.373016</td>\n",
              "      <td>0.951818</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.729492</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.485507</td>\n",
              "      <td>0.451389</td>\n",
              "      <td>0.779221</td>\n",
              "      <td>0.644231</td>\n",
              "      <td>0.607477</td>\n",
              "      <td>0.421769</td>\n",
              "      <td>0.393333</td>\n",
              "      <td>0.359477</td>\n",
              "      <td>0.579439</td>\n",
              "      <td>0.551402</td>\n",
              "      <td>0.514019</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.218750</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>1.037632</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.703875</td>\n",
              "      <td>0.548889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the right of a government arbitrarily to set a...</td>\n",
              "      <td>the right for a government to draw aside its c...</td>\n",
              "      <td>4.80</td>\n",
              "      <td>0.243478</td>\n",
              "      <td>0.997143</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.746250</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.734940</td>\n",
              "      <td>0.592920</td>\n",
              "      <td>0.512195</td>\n",
              "      <td>0.859155</td>\n",
              "      <td>0.752809</td>\n",
              "      <td>0.684783</td>\n",
              "      <td>0.434109</td>\n",
              "      <td>0.365672</td>\n",
              "      <td>0.302158</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.544444</td>\n",
              "      <td>0.471910</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.208333</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.643078</td>\n",
              "      <td>0.863771</td>\n",
              "      <td>0.788657</td>\n",
              "      <td>0.771111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the house have also fight however for the redu...</td>\n",
              "      <td>this parliament have also fight for this reduc...</td>\n",
              "      <td>4.00</td>\n",
              "      <td>0.079812</td>\n",
              "      <td>0.966302</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.475191</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.604027</td>\n",
              "      <td>0.338521</td>\n",
              "      <td>0.254967</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.450777</td>\n",
              "      <td>0.361502</td>\n",
              "      <td>0.223602</td>\n",
              "      <td>0.189759</td>\n",
              "      <td>0.169643</td>\n",
              "      <td>0.325792</td>\n",
              "      <td>0.285068</td>\n",
              "      <td>0.259091</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.105263</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.088235</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.358974</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.652174</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>1.673213</td>\n",
              "      <td>0.493735</td>\n",
              "      <td>0.687988</td>\n",
              "      <td>0.442094</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Sen1  ... soft stopword\n",
              "0  in nigeria chevron have be accuse by the allij...  ...      0.674476\n",
              "1  i know that in france they have have whole her...  ...      0.644252\n",
              "2  unfortunately the ultimate objective of a euro...  ...      0.548889\n",
              "3  the right of a government arbitrarily to set a...  ...      0.771111\n",
              "4  the house have also fight however for the redu...  ...      0.442094\n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 475
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "Kd04N6IGjTCi",
        "outputId": "bc90d6dd-6a5f-4aca-f666-28fa4d821573"
      },
      "source": [
        "data_test.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sen1</th>\n",
              "      <th>Sen2</th>\n",
              "      <th>gs</th>\n",
              "      <th>LCStr</th>\n",
              "      <th>soft preprocessed</th>\n",
              "      <th>JS preprocessed</th>\n",
              "      <th>soft synsets</th>\n",
              "      <th>JS Synsets</th>\n",
              "      <th>char 2 gram jac</th>\n",
              "      <th>char 3 gram jac</th>\n",
              "      <th>char 4 gram jac</th>\n",
              "      <th>char 2 gram cont</th>\n",
              "      <th>char 3 gram cont</th>\n",
              "      <th>char 4 gram cont</th>\n",
              "      <th>char 5 gram jac</th>\n",
              "      <th>char 6 gram jac</th>\n",
              "      <th>char 7 gram jac</th>\n",
              "      <th>char 5 gram cont</th>\n",
              "      <th>char 6 gram cont</th>\n",
              "      <th>char 7 gram cont</th>\n",
              "      <th>Con 1gram w/o stw</th>\n",
              "      <th>Con 2gram w/o stw</th>\n",
              "      <th>Con 3gram w/o stw</th>\n",
              "      <th>Con 4gram w/o stw</th>\n",
              "      <th>JS 1gram</th>\n",
              "      <th>JS 3gram</th>\n",
              "      <th>JS 4gram</th>\n",
              "      <th>JS 1gram w/o stw</th>\n",
              "      <th>JS 3gram w/o stw</th>\n",
              "      <th>JS 4gram w/o stw</th>\n",
              "      <th>POS 1gram jac</th>\n",
              "      <th>POS 2gram jac</th>\n",
              "      <th>POS 3gram jac</th>\n",
              "      <th>POS 1gram cont</th>\n",
              "      <th>POS 2gram cont</th>\n",
              "      <th>POS 3gram cont</th>\n",
              "      <th>res dif</th>\n",
              "      <th>lin</th>\n",
              "      <th>wup</th>\n",
              "      <th>soft stopword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the leader have now be give a new chance and l...</td>\n",
              "      <td>the leader benefit aujourd hui of a new luck a...</td>\n",
              "      <td>4.50</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.806667</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.377382</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.291667</td>\n",
              "      <td>0.193548</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.488372</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.269231</td>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.067961</td>\n",
              "      <td>0.038462</td>\n",
              "      <td>0.196078</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.081633</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.347826</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.368421</td>\n",
              "      <td>0.130435</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.470321</td>\n",
              "      <td>0.424578</td>\n",
              "      <td>0.478894</td>\n",
              "      <td>0.526738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>amendment no 7 proposes certain change in the ...</td>\n",
              "      <td>amendment no 7 be propose certain change in th...</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.954904</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.846678</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.816667</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.907407</td>\n",
              "      <td>0.872727</td>\n",
              "      <td>0.703125</td>\n",
              "      <td>0.646154</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.792453</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.848571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>let me remind you that our ally include ferven...</td>\n",
              "      <td>i would like to remind you that among our ally...</td>\n",
              "      <td>4.25</td>\n",
              "      <td>0.229730</td>\n",
              "      <td>0.816779</td>\n",
              "      <td>0.708333</td>\n",
              "      <td>0.521682</td>\n",
              "      <td>0.380952</td>\n",
              "      <td>0.414286</td>\n",
              "      <td>0.279070</td>\n",
              "      <td>0.224719</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.436364</td>\n",
              "      <td>0.370370</td>\n",
              "      <td>0.186813</td>\n",
              "      <td>0.152174</td>\n",
              "      <td>0.118280</td>\n",
              "      <td>0.320755</td>\n",
              "      <td>0.269231</td>\n",
              "      <td>0.215686</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.380952</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.583386</td>\n",
              "      <td>0.533131</td>\n",
              "      <td>0.630757</td>\n",
              "      <td>0.453704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the vote will take place today at 530 pm</td>\n",
              "      <td>the vote will take place at 530pm</td>\n",
              "      <td>4.50</td>\n",
              "      <td>0.757576</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>0.700472</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.781250</td>\n",
              "      <td>0.718750</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0.806452</td>\n",
              "      <td>0.766667</td>\n",
              "      <td>0.724138</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>0.531250</td>\n",
              "      <td>0.468750</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.629630</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.496858</td>\n",
              "      <td>0.812678</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.433036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the fisherman be inactive tired and disappointed</td>\n",
              "      <td>the fisherman be inactive tired and disappointed</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996471</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.991429</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.985000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Sen1  ... soft stopword\n",
              "0  the leader have now be give a new chance and l...  ...      0.526738\n",
              "1  amendment no 7 proposes certain change in the ...  ...      0.848571\n",
              "2  let me remind you that our ally include ferven...  ...      0.453704\n",
              "3           the vote will take place today at 530 pm  ...      0.433036\n",
              "4   the fisherman be inactive tired and disappointed  ...      0.985000\n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 476
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jPsvoDQga1H"
      },
      "source": [
        "# Feature combination\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzZylMUPfZOH"
      },
      "source": [
        "##########################################################################################################\n",
        "                                  # Prepare data for model\n",
        "##########################################################################################################\n",
        "\n",
        "X_train = data_train.drop(['Sen1', 'Sen2'], axis=1)\n",
        "corr_train = X_train.corr()\n",
        "X_train = X_train.drop('gs', axis=1)\n",
        "Y_train = data_train['gs']\n",
        "\n",
        "X_test = data_test.drop(['Sen1', 'Sen2'], axis=1)\n",
        "corr_test = X_test.corr()\n",
        "X_test = X_test.drop('gs', axis=1)\n",
        "Y_test = data_test['gs'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aakgbNufcHM",
        "outputId": "4d7bea8c-c30b-4893-f745-c100f6ff28e5"
      },
      "source": [
        "##########################################################################################################\n",
        "                          # Algorithm selection, model fitting and prediction\n",
        "##########################################################################################################\n",
        "\n",
        "\n",
        "classifier = svm.SVR()\n",
        "\n",
        "classifier.fit(X_train, Y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "corr = pearsonr(Y_test,predictions)[0]\n",
        "print(classifier , ':',corr)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVR() : 0.7773784720051551\n"
          ]
        }
      ]
    }
  ]
}